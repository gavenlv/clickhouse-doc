# å¸¸è§ç›‘æ§é…ç½®

æœ¬æ–‡æ¡£æä¾›äº† ClickHouse ç›‘æ§çš„å¸¸è§é…ç½®ç¤ºä¾‹ï¼Œæ¶µç›–åŸºç¡€ç›‘æ§ã€ä¼ä¸šçº§ç›‘æ§å’Œé«˜å¯ç”¨ç›‘æ§ã€‚

## ğŸ”§ åŸºç¡€ç›‘æ§é…ç½®

### 1. æŸ¥è¯¢æ—¥å¿—é…ç½®

#### å¯ç”¨æŸ¥è¯¢æ—¥å¿—

```xml
<!-- config.xml -->
<clickhouse>
    <!-- æŸ¥è¯¢æ—¥å¿—é…ç½® -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <!-- æŒ‰æ—¥æœŸåˆ†åŒº -->
        <partition_by>toYYYYMM(event_date)</partition_by>
        <!-- TTL: ä¿ç•™ 30 å¤© -->
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <!-- è®°å½•æ‰€æœ‰æŸ¥è¯¢ç±»å‹ -->
        <type>1</type>
        <!-- è®°å½•é—´éš”ï¼š0 è¡¨ç¤ºè®°å½•æ‰€æœ‰æŸ¥è¯¢ -->
        <interval_milliseconds>0</interval_milliseconds>
    </query_log>

    <!-- æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½® -->
    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
    </query_thread_log>

    <!-- é”™è¯¯æ—¥å¿—é…ç½® -->
    <text_log>
        <level>warning</level>
        <database>system</database>
        <table>text_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 90 DAY DELETE</ttl>
    </text_log>
</clickhouse>
```

#### æŸ¥è¯¢æ—¥å¿—ä¼˜åŒ–

```xml
<!-- æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘æ—¥å¿—è®°å½•é‡ -->
<clickhouse>
    <!-- åªè®°å½•æ…¢æŸ¥è¯¢ï¼ˆè¶…è¿‡ 1 ç§’ï¼‰ -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <!-- åªè®°å½• QueryFinish -->
        <type>2</type>
        <!-- ä¸è®°å½•å†…éƒ¨æŸ¥è¯¢ -->
        <remove_unnecessary_records>true</remove_unnecessary_records>
    </query_log>

    <!-- é‡‡æ ·è®°å½•ï¼šè®°å½• 10% çš„æŸ¥è¯¢ -->
    <trace_log>
        <database>system</database>
        <table>trace_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 7 DAY DELETE</ttl>
        <sampling>0.1</sampling>
    </trace_log>
</clickhouse>
```

### 2. å¼‚æ­¥æŒ‡æ ‡é…ç½®

```xml
<!-- å¼‚æ­¥æŒ‡æ ‡é…ç½® -->
<clickhouse>
    <!-- å¼‚æ­¥æŒ‡æ ‡æ”¶é›†é—´éš”ï¼šé»˜è®¤ 1 ç§’ -->
    <asynchronous_metrics_log>
        <database>system</database>
        <table>asynchronous_metrics_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
    </asynchronous_metrics_log>

    <!-- å¼‚æ­¥æŒ‡æ ‡å†å²ï¼šä»…ä¿ç•™æœ€è¿‘æ•°æ® -->
    <asynchronous_metric_history>
        <database>system</database>
        <table>asynchronous_metric_history</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 7 DAY DELETE</ttl>
    </asynchronous_metric_history>
</clickhouse>
```

### 3. åŸºç¡€ç›‘æ§è§†å›¾

```sql
-- åˆ›å»ºç›‘æ§æ•°æ®åº“
CREATE DATABASE IF NOT EXISTS monitoring;

-- åˆ›å»ºåŸºç¡€ç›‘æ§è§†å›¾
CREATE VIEW monitoring.basic_metrics AS
SELECT
    now() AS timestamp,
    'CPU' AS metric_type,
    'Usage' AS metric_name,
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSCPUVirtualTimeMicroseconds') AS value
UNION ALL
SELECT
    now() AS timestamp,
    'Memory' AS metric_type,
    'Active' AS metric_name,
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSMemoryActive') AS value
UNION ALL
SELECT
    now() AS timestamp,
    'Memory' AS metric_type,
    'Total' AS metric_name,
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSMemoryTotal') AS value
UNION ALL
SELECT
    now() AS timestamp,
    'Disk' AS metric_type,
    'Available' AS metric_name,
    available_space AS value
FROM system.disks;
```

## ğŸ”§ ä¼ä¸šçº§ç›‘æ§é…ç½®

### 1. å®Œæ•´æ—¥å¿—é…ç½®

```xml
<!-- å®Œæ•´çš„ä¼ä¸šçº§æ—¥å¿—é…ç½® -->
<clickhouse>
    <!-- æŸ¥è¯¢æ—¥å¿— -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <type>1,2,4</type>
        <remove_unnecessary_records>true</remove_unnecessary_records>
    </query_log>

    <!-- æŸ¥è¯¢çº¿ç¨‹æ—¥å¿— -->
    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
    </query_thread_log>

    <!-- å¼‚æ­¥æŒ‡æ ‡æ—¥å¿— -->
    <asynchronous_metrics_log>
        <database>system</database>
        <table>asynchronous_metrics_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
    </asynchronous_metrics_log>

    <!-- äº‹ä»¶æ—¥å¿— -->
    <event_log>
        <database>system</database>
        <table>event_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
    </event_log>

    <!-- ç³»ç»Ÿæ—¥å¿— -->
    <text_log>
        <level>information</level>
        <database>system</database>
        <table>text_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 90 DAY DELETE</ttl>
    </text_log>

    <!-- Mutation æ—¥å¿— -->
    <mutation_log>
        <database>system</database>
        <table>mutation_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 90 DAY DELETE</ttl>
    </mutation_log>

    <!-- ä¼šè¯æ—¥å¿— -->
    <session_log>
        <database>system</database>
        <table>session_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
    </session_log>

    <!-- ZooKeeper æ—¥å¿— -->
    <zookeeper_log>
        <database>system</database>
        <table>zookeeper_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
    </zookeeper_log>
</clickhouse>
```

### 2. ä¼ä¸šçº§ç›‘æ§è§†å›¾

```sql
-- åˆ›å»ºç›‘æ§æ•°æ®åº“
CREATE DATABASE IF NOT EXISTS monitoring;

-- ç³»ç»Ÿèµ„æºç›‘æ§è§†å›¾
CREATE VIEW monitoring.system_resources AS
SELECT
    now() AS timestamp,
    'CPU' AS resource_type,
    'Usage' AS metric_name,
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSCPUVirtualTimeMicroseconds') AS value,
    '%' AS unit
UNION ALL
SELECT
    now() AS timestamp,
    'Memory' AS resource_type,
    'Usage' AS metric_name,
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSMemoryActive') * 100.0 /
    (SELECT value FROM system.asynchronous_metrics WHERE metric = 'OSMemoryTotal') AS value,
    '%' AS unit
UNION ALL
SELECT
    now() AS timestamp,
    'Disk' AS resource_type,
    'Usage' AS metric_name,
    (total_space - available_space) * 100.0 / total_space AS value,
    '%' AS unit
FROM system.disks;

-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§è§†å›¾
CREATE VIEW monitoring.query_performance AS
SELECT
    toStartOfMinute(event_time) AS minute,
    count() AS total_queries,
    countIf(query_duration_ms < 100) AS very_fast_queries,
    countIf(query_duration_ms >= 100 AND query_duration_ms < 1000) AS fast_queries,
    countIf(query_duration_ms >= 1000 AND query_duration_ms < 5000) AS normal_queries,
    countIf(query_duration_ms >= 5000) AS slow_queries,
    avg(query_duration_ms) AS avg_duration_ms,
    max(query_duration_ms) AS max_duration_ms,
    sum(read_bytes) AS total_read_bytes,
    sum(memory_usage) AS total_memory_usage
FROM system.query_log
WHERE type = 'QueryFinish'
  AND event_date >= today() - INTERVAL 7 DAY
GROUP BY minute;

-- è¡¨å¥åº·ç›‘æ§è§†å›¾
CREATE VIEW monitoring.table_health AS
SELECT
    database,
    table,
    engine,
    total_rows,
    total_bytes,
    formatReadableSize(total_bytes) AS readable_size,
    count() AS part_count,
    avg(rows) AS avg_rows_per_part,
    avg(bytes_on_disk) AS avg_bytes_per_part,
    CASE
        WHEN engine ILIKE '%Replicated%' THEN 'YES'
        ELSE 'NO'
    END AS is_replicated,
    CASE
        WHEN engine ILIKE '%Replicated%' THEN 'OK'
        WHEN total_bytes > 10737418240 THEN 'WARNING'
        ELSE 'OK'
    END AS status
FROM system.tables AS t
LEFT JOIN (
    SELECT
        database,
        table,
        sum(rows) AS rows,
        sum(bytes_on_disk) AS bytes_on_disk
    FROM system.parts
    WHERE active
    GROUP BY database, table
) AS p ON t.database = p.database AND t.table = p.table
WHERE t.database NOT IN ('system', 'information_schema', 'INFORMATION_SCHEMA')
GROUP BY database, table, engine, total_rows, total_bytes
ORDER BY total_bytes DESC;
```

### 3. å‘Šè­¦é…ç½®è¡¨

```sql
-- åˆ›å»ºå‘Šè­¦é…ç½®è¡¨
CREATE TABLE IF NOT EXISTS monitoring.alert_config (
    id UInt64,
    category String,        -- System, Query, DataQuality, Operation
    resource String,        -- CPU, Memory, Disk, Query, etc.
    alert_type String,      -- High Usage, Low Space, Slow Query, etc.
    level String,           -- CRITICAL, WARNING, INFO
    threshold String,        -- å‘Šè­¦é˜ˆå€¼
    duration_interval UInt32, -- æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    enabled UInt8,           -- æ˜¯å¦å¯ç”¨
    description String,
    created_at DateTime DEFAULT now(),
    updated_at DateTime DEFAULT now()
)
ENGINE = MergeTree()
ORDER BY (id);

-- æ’å…¥å‘Šè­¦é…ç½®
INSERT INTO monitoring.alert_config (id, category, resource, alert_type, level, threshold, duration_interval, enabled, description) VALUES
-- ç³»ç»Ÿå‘Šè­¦
(1, 'System', 'CPU', 'High Usage', 'WARNING', '80%', 600, 1, 'CPU ä½¿ç”¨ç‡è¶…è¿‡ 80% æŒç»­ 10 åˆ†é’Ÿ'),
(2, 'System', 'CPU', 'Critical Usage', 'CRITICAL', '90%', 300, 1, 'CPU ä½¿ç”¨ç‡è¶…è¿‡ 90% æŒç»­ 5 åˆ†é’Ÿ'),
(3, 'System', 'Memory', 'High Usage', 'WARNING', '85%', 300, 1, 'å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 85% æŒç»­ 5 åˆ†é’Ÿ'),
(4, 'System', 'Memory', 'Critical Usage', 'CRITICAL', '95%', 60, 1, 'å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 95% æŒç»­ 1 åˆ†é’Ÿ'),
(5, 'System', 'Disk', 'Low Space', 'WARNING', '20%', 3600, 1, 'ç£ç›˜å¯ç”¨ç©ºé—´ä½äº 20%'),
(6, 'System', 'Disk', 'Critical Low Space', 'CRITICAL', '10%', 1800, 1, 'ç£ç›˜å¯ç”¨ç©ºé—´ä½äº 10%'),

-- æŸ¥è¯¢å‘Šè­¦
(7, 'Query', 'Performance', 'Slow Query', 'WARNING', '30s', 60, 1, 'æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¶…è¿‡ 30 ç§’'),
(8, 'Query', 'Performance', 'Very Slow Query', 'CRITICAL', '300s', 60, 1, 'æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¶…è¿‡ 5 åˆ†é’Ÿ'),
(9, 'Query', 'Memory', 'High Memory Usage', 'WARNING', '1GB', 60, 1, 'æŸ¥è¯¢å†…å­˜ä½¿ç”¨è¶…è¿‡ 1GB'),
(10, 'Query', 'Memory', 'Very High Memory Usage', 'CRITICAL', '4GB', 60, 1, 'æŸ¥è¯¢å†…å­˜ä½¿ç”¨è¶…è¿‡ 4GB'),

-- æ•°æ®è´¨é‡å‘Šè­¦
(11, 'DataQuality', 'Partition', 'Partition Skew', 'WARNING', '3', 3600, 1, 'åˆ†åŒºå€¾æ–œåº¦è¶…è¿‡ 3'),
(12, 'DataQuality', 'Partition', 'Severe Partition Skew', 'CRITICAL', '10', 1800, 1, 'åˆ†åŒºå€¾æ–œåº¦è¶…è¿‡ 10'),
(13, 'DataQuality', 'Replication', 'Non-replicated Table', 'WARNING', '10GB', 0, 1, 'å­˜åœ¨è¶…è¿‡ 10GB çš„éå¤åˆ¶è¡¨'),

-- æ“ä½œå‘Šè­¦
(14, 'Operation', 'ALTER', 'Frequent ALTER', 'WARNING', '10/hour', 3600, 1, '1 å°æ—¶å†… ALTER æ“ä½œè¶…è¿‡ 10 æ¬¡'),
(15, 'Operation', 'DELETE', 'Large Delete', 'WARNING', '1M rows', 0, 1, 'DELETE æ“ä½œè¶…è¿‡ 100 ä¸‡è¡Œ'),

-- é›†ç¾¤å‘Šè­¦
(16, 'Cluster', 'Replica', 'Replica Lag', 'WARNING', '1800s', 300, 1, 'å‰¯æœ¬å»¶è¿Ÿè¶…è¿‡ 30 åˆ†é’Ÿ'),
(17, 'Cluster', 'Replica', 'Critical Replica Lag', 'CRITICAL', '3600s', 300, 1, 'å‰¯æœ¬å»¶è¿Ÿè¶…è¿‡ 1 å°æ—¶'),
(18, 'Cluster', 'ZooKeeper', 'Connection Lost', 'CRITICAL', '0', 0, 1, 'ZooKeeper è¿æ¥ä¸¢å¤±');
```

## ğŸ”§ é«˜å¯ç”¨ç›‘æ§é…ç½®

### 1. Prometheus é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # ClickHouse ä¸»èŠ‚ç‚¹ç›‘æ§
  - job_name: 'clickhouse-primary'
    static_configs:
      - targets: ['clickhouse-primary:9363']
        labels:
          cluster: 'treasurycluster'
          role: 'primary'

  # ClickHouse å‰¯æœ¬èŠ‚ç‚¹ç›‘æ§
  - job_name: 'clickhouse-replica'
    static_configs:
      - targets:
          - 'clickhouse-replica1:9363'
          - 'clickhouse-replica2:9363'
        labels:
          cluster: 'treasurycluster'
          role: 'replica'

  # ClickHouse ZooKeeper ç›‘æ§
  - job_name: 'zookeeper'
    static_configs:
      - targets:
          - 'zookeeper1:9363'
          - 'zookeeper2:9363'
          - 'zookeeper3:9363'
        labels:
          cluster: 'treasurycluster'
          component: 'zookeeper'
```

### 2. Grafana ä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "ClickHouse Cluster Monitoring",
    "panels": [
      {
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(OSCPUVirtualTimeMicroseconds[5m])",
            "legendFormat": "{{instance}}"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "OSMemoryActive / OSMemoryTotal * 100",
            "legendFormat": "{{instance}}"
          }
        ]
      },
      {
        "title": "Query Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(QueryDurationMs_bucket[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(QueryDurationMs_bucket[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "Replication Lag",
        "type": "graph",
        "targets": [
          {
            "expr": "ReplicaQueueAbsoluteDelay",
            "legendFormat": "{{database}}.{{table}}"
          }
        ]
      }
    ]
  }
}
```

### 3. é«˜å¯ç”¨ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# high_availability_monitor.sh

# é…ç½®
CLICKHOUSE_HOSTS=("clickhouse-primary:9000" "clickhouse-replica1:9000" "clickhouse-replica2:9000")
ALERT_WEBHOOK="https://your-webhook-url/alert"
CLUSTER_NAME="treasurycluster"

# æ£€æŸ¥èŠ‚ç‚¹å¯ç”¨æ€§
check_node_health() {
    local host=$1
    if clickhouse-client --host "$host" --query "SELECT 1" >/dev/null 2>&1; then
        echo "OK"
    else
        echo "CRITICAL"
    fi
}

# æ£€æŸ¥å‰¯æœ¬å»¶è¿Ÿ
check_replica_lag() {
    local host=$1
    local lag=$(clickhouse-client --host "$host" --query "
        SELECT max(absolute_delay)
        FROM system.replication_queue
        WHERE database NOT IN ('system')
        GROUP BY database, table
    " 2>/dev/null)

    if [ -z "$lag" ]; then
        echo "0"
    else
        echo "$lag"
    fi
}

# ä¸»ç›‘æ§å¾ªç¯
while true; do
    timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

    for host in "${CLICKHOUSE_HOSTS[@]}"; do
        # æ£€æŸ¥èŠ‚ç‚¹å¥åº·
        health=$(check_node_health "$host")

        # æ£€æŸ¥å‰¯æœ¬å»¶è¿Ÿ
        lag=$(check_replica_lag "$host")

        # å‘é€å‘Šè­¦
        if [ "$health" = "CRITICAL" ]; then
            curl -X POST "$ALERT_WEBHOOK" \
                 -H "Content-Type: application/json" \
                 -d "{
                   \"timestamp\": \"$timestamp\",
                   \"cluster\": \"$CLUSTER_NAME\",
                   \"host\": \"$host\",
                   \"alert_type\": \"NodeDown\",
                   \"level\": \"CRITICAL\"
                 }"
        fi

        if [ "$lag" -gt 3600 ]; then
            curl -X POST "$ALERT_WEBHOOK" \
                 -H "Content-Type: application/json" \
                 -d "{
                   \"timestamp\": \"$timestamp\",
                   \"cluster\": \"$CLUSTER_NAME\",
                   \"host\": \"$host\",
                   \"alert_type\": \"ReplicaLag\",
                   \"level\": \"WARNING\",
                   \"value\": \"$lag\"
                 }"
        fi
    done

    sleep 60
done
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–ç›‘æ§é…ç½®

### 1. é‡‡æ ·é…ç½®

```xml
<!-- æ€§èƒ½ä¼˜åŒ–ï¼šé‡‡æ ·é…ç½® -->
<clickhouse>
    <!-- æŸ¥è¯¢æ—¥å¿—é‡‡æ ·ï¼šè®°å½• 10% çš„æŸ¥è¯¢ -->
    <query_log>
        <database>system</database>
        <table>query_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 30 DAY DELETE</ttl>
        <!-- é‡‡æ ·ç‡ï¼š0.1 = 10% -->
        <sampling>0.1</sampling>
    </query_log>

    <!-- Trace æ—¥å¿—é‡‡æ ·ï¼šè®°å½• 1% çš„æŸ¥è¯¢ -->
    <trace_log>
        <database>system</database>
        <table>trace_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <ttl>event_date + INTERVAL 7 DAY DELETE</ttl>
        <sampling>0.01</sampling>
    </trace_log>
</clickhouse>
```

### 2. é¢„èšåˆé…ç½®

```sql
-- åˆ›å»ºé¢„èšåˆç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW monitoring.query_stats_hourly
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (date, hour, user)
AS SELECT
    toDate(event_time) AS date,
    toHour(event_time) AS hour,
    user,
    count() AS query_count,
    sum(query_duration_ms) AS total_duration_ms,
    sum(read_rows) AS total_read_rows,
    sum(read_bytes) AS total_read_bytes,
    sum(memory_usage) AS total_memory_usage
FROM system.query_log
WHERE type = 'QueryFinish'
GROUP BY date, hour, user;

-- åˆ›å»ºæ…¢æŸ¥è¯¢é¢„èšåˆ
CREATE MATERIALIZED VIEW monitoring.slow_query_stats_hourly
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (date, hour, user)
AS SELECT
    toDate(event_time) AS date,
    toHour(event_time) AS hour,
    user,
    count() AS slow_query_count,
    avg(query_duration_ms) AS avg_duration_ms,
    max(query_duration_ms) AS max_duration_ms
FROM system.query_log
WHERE type = 'QueryFinish'
  AND query_duration_ms > 5000
GROUP BY date, hour, user;
```

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **é…ç½®æµ‹è¯•**: åœ¨ç”Ÿäº§ç¯å¢ƒåº”ç”¨é…ç½®å‰å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯
2. **æ€§èƒ½å½±å“**: ç›‘æ§é…ç½®ä¼šå½±å“æ€§èƒ½ï¼Œéœ€è¦æƒè¡¡ç›‘æ§ç²’åº¦
3. **å­˜å‚¨ç©ºé—´**: æ—¥å¿—ä¼šå ç”¨å¤§é‡å­˜å‚¨ï¼Œåˆç†è®¾ç½® TTL
4. **é…ç½®å¤‡ä»½**: å®šæœŸå¤‡ä»½é…ç½®æ–‡ä»¶
5. **ç‰ˆæœ¬å…¼å®¹**: é…ç½®å¯èƒ½å›  ClickHouse ç‰ˆæœ¬è€Œå¼‚
6. **æ–‡æ¡£æ›´æ–°**: åŠæ—¶æ›´æ–°ç›‘æ§é…ç½®æ–‡æ¡£
7. **å®šæœŸå®¡æŸ¥**: å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–ç›‘æ§é…ç½®
8. **æƒé™æ§åˆ¶**: ç›‘æ§é…ç½®åº”è¯¥æœ‰ä¸¥æ ¼çš„è®¿é—®æ§åˆ¶

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [01_system_monitoring.md](./01_system_monitoring.md) - ç³»ç»Ÿç›‘æ§
- [02_query_monitoring.md](./02_query_monitoring.md) - æŸ¥è¯¢ç›‘æ§
- [06_alerting.md](./06_alerting.md) - å‘Šè­¦æœºåˆ¶
- [06-admin/](../06-admin/) - è¿ç»´ç®¡ç†
